#! /usr/bin/python2.7 
# -*- coding: utf8 -*-

import sys
import os, subprocess, shlex, re, gzip
# Just for DEBUG
#os.chdir('/Users/sofiasilva/GitHub/cool_bgp_stats')
from get_file import get_file
import bgp_rib
import pickle
import radix
from calendar import timegm
from datetime import datetime, date, timedelta
import pandas as pd
from VisibilityDBHandler import VisibilityDBHandler
from time import time
import resource
from netaddr import IPNetwork

# For some reason in my computer os.getenv('PATH') differs from echo $PATH
# /usr/local/bin is not in os.getenv('PATH')
# it also works in matong
bgpdump = '/usr/local/bin/bgpdump'

class BGPDataHandler:
    DEBUG = False
    files_path = ''
  
    # STRUCTURES WITH CURRENT ROUTING DATA
  
    # DataFrame with routing data from routing files
    bgp_df = pd.DataFrame()
    
    # Radix indexed by routed IPv4 prefix containing the routing data from the
    # routing file being considered
    ipv4Prefixes_radix = radix.Radix()

    # Radix indexed by routed IPv6 prefix containing the routing data from the
    # routing file being considered
    ipv6Prefixes_radix = radix.Radix()

    # Numeric variable with the longest IPv4 prefix length
    ipv4_longest_pref = -1

    # Numeric variable with the longest IPv6 prefix length
    ipv6_longest_pref = -1  
    
    # DataFrame with BGP updates from bgpupd file
    updates_df = pd.DataFrame()
    
    routingDate = ''
    updatesDate = ''
         
    # When we instantiate this class we set a boolean variable specifying
    # whether we will be working on DEBUG mode, we also set the variable with
    # the path to the folder we will use to store files (files_path) and
    # we set a boolean variable specifying whether we want to KEEP the
    # intermediate files generated by different functions
    def __init__(self, DEBUG, files_path):
        self.DEBUG = DEBUG
        self.files_path = files_path
        self.output_file = '{}/BGPDataHandler.output'.format(files_path)

        sys.stdout.write("BGPDataHandler instantiated successfully! Remember to load the data structures.\n")
    
   
    # This function loads the data structures of the class from previously
    # generated pickle files containing the result of already processed routing data
    def loadStructuresFromFiles(self, r_date, u_date, bgp_df_file,\
                                updates_df_file, ipv4_prefixes_file,\
                                ipv6_prefixes_file):
     
        self.routingDate = r_date
        self.updatesDate = u_date
        self.bgp_df = pickle.load(open(bgp_df_file, "rb"))
        self.updates_df = pickle.load(open(updates_df_file, "rb"))
        self.ipv4Prefixes_radix = pickle.load(open(ipv4_prefixes_file, "rb"))
        self.ipv6Prefixes_radix = pickle.load(open(ipv6_prefixes_file, "rb"))
        self.setLongestPrefixLengths()
        sys.stdout.write("Class data structures were loaded successfully!\n")
        return True
    
    # This function processes the routing data contained in the files to which
    # the URLs in the urls_file point, and loads the data structures of the class
    # with the results from this processing
    def loadStructuresFromURLSfile(self, urls_file):
        files_date, update_files_date, bgp_df, updates_df,\
            ipv4Prefixes_radix, ipv6Prefixes_radix,\
            ipv4_longest_pref, ipv6_longest_pref  =\
                        self.processMultipleFiles(files_list=urls_file,\
                                                isList=False, containsURLs=True)
        
        aux_date = datetime.strptime('1970', '%Y').date()
        
        if files_date != aux_date:
            self.routingDate = files_date

        if update_files_date != aux_date:
            self.updatesDate = update_files_date

        if bgp_df.shape[0] != 0:
            self.bgp_df = bgp_df

        if updates_df.shape[0] != 0:
            self.updates_df = updates_df
            
        if len(ipv4Prefixes_radix.prefixes()) != 0:
            self.ipv4Prefixes_radix = ipv4Prefixes_radix
        
        if len(ipv6Prefixes_radix.prefixes()) != 0:
            self.ipv6Prefixes_radix = ipv6Prefixes_radix
        
        if ipv4_longest_pref != -1:
            self.ipv4_longest_pref = ipv4_longest_pref
        else:
            self.ipv4_longest_pref = 32

        if ipv6_longest_pref != -1:
            self.ipv6_longest_pref = ipv6_longest_pref
        else:
            self.ipv6_longest_pref = 64

        sys.stdout.write("Class data structures from files in URLs file were loaded successfully!\n")
        return True

    # This function processes the updates from the updates file and loads
    # the corresponding variables of the class with the results from this processing
    def loadStructuresFromUpdatesFile(self, updates_file):
        if not updates_file.endswith('readable'):
            readable_file_name = self.getReadableFile(updates_file, False, True, False)
        else:
            readable_file_name = updates_file
            
        updates_df = pd.DataFrame()
        
        if readable_file_name != '':
            updates_file_date, updates_df =\
                                self.processReadableUpdatesDF(readable_file_name)
            
            self.updatesDate = updates_file_date
            self.updates_df = updates_df
            
            sys.stdout.write("Class data structures were loaded successfully with info from updates file!\n")
            return True
        else:
            sys.stderr.write("Could not process updates file.\n")
            return False
            
    # This function processes the routing data contained in the routing_file
    # and loads the corresponding variables of the class with
    # the results from this processing                                           
    def loadStructuresFromRoutingFile(self, routing_file):
        if not routing_file.endswith('readable'):
            readable_file_name =  self.getReadableFile(routing_file, False)
        else:
            readable_file_name = routing_file
        
        bgp_df = pd.DataFrame()
        ipv4Prefixes_radix = radix.Radix()
        ipv6Prefixes_radix = radix.Radix()
        
        if readable_file_name != '':
            routing_date, bgp_df, ipv4Prefixes_radix, ipv6Prefixes_radix,\
                ipv4_longest_pref, ipv6_longest_pref =\
                                    self.processReadableDF(readable_file_name,
                                                           bgp_df,
                                                           ipv4Prefixes_radix,
                                                           ipv6Prefixes_radix)
        
            self.routingDate = routing_date 
            self.bgp_df = bgp_df             
            self.ipv4Prefixes_radix = ipv4Prefixes_radix
            self.ipv6Prefixes_radix = ipv6Prefixes_radix
            
            if ipv4_longest_pref != -1:
                self.ipv4_longest_pref = ipv4_longest_pref
            else:
                self.ipv4_longest_pref = 32
            if ipv6_longest_pref != -1:
                self.ipv6_longest_pref = ipv6_longest_pref
            else:
                self.ipv6_longest_pref = 64
    
            sys.stdout.write("Class data structures were loaded successfully with info from routing file!\n")
            return True
        else:
            sys.stderr.write("Could not process routing file.\n")
            return False


    # This function loads the data structures of the class with the routing
    # data contained in the archive folder corresponding to the routing
    # date provided or to the most recent date present in the archive
    def loadStructuresFromArchive(self, archive_folder='/data/wattle/bgplog', extension='',
                                  routing_date=''):
        
        if routing_date == '':
            date_files  =\
                        self.getMostRecentsFromArchive(archive_folder,
                                                       extension, [])
        
            if len(date_files) == 0:
                if extension == '':
                    sys.stderr.write("There are no files in the archive!\n".format(extension))
                else:
                    sys.stderr.write("There are no files with extension {} in the archive!\n".format(extension))
                return False
                
        else:
            routing_files = self.getSpecificFilesFromArchive(routing_date,
                                                             archive_folder,
                                                             extension, [])
                                                          
            if len(routing_files) == 0:
                sys.stderr.write("There is no routing file in the archive for the date provided.\n")
                return False
                    
        files_date, update_files_date, bgp_df, updates_df,\
            ipv4Prefixes_radix, ipv6Prefixes_radix,\
            ipv4_longest_pref, ipv6_longest_pref  =\
                        self.processMultipleFiles(files_list=routing_files,
                                                isList=True, containsURLs=False)
        aux_date = datetime.strptime('1970', '%Y').date()

        if files_date != aux_date:                    
            self.routingDate = files_date
        
        if update_files_date != aux_date:
            self.updatesDate = update_files_date

        if bgp_df.shape[0] != 0:
            self.bgp_df = bgp_df
        
        if updates_df.shape[0] != 0:
            self.updates_df = updates_df
            
        if len(ipv4Prefixes_radix.prefixes()) != 0:
            self.ipv4Prefixes_radix = ipv4Prefixes_radix
            
        if len(ipv6Prefixes_radix.prefixes()) != 0:
            self.ipv6Prefixes_radix = ipv6Prefixes_radix
        
        if ipv4_longest_pref != -1:
            self.ipv4_longest_pref = ipv4_longest_pref
        else:
            self.ipv4_longest_pref = 32

        if ipv6_longest_pref != -1:
            self.ipv6_longest_pref = ipv6_longest_pref
        else:
            self.ipv6_longest_pref = 64

        sys.stdout.write("Class data structures were loaded successfully!\n")
            
        return True
        

    def storeHistoricalDataFromArchive(self, startDate, endDate,
                                        archive_folder='/data/wattle/bgplog',
                                        extension=''):

        historical_files = self.getPathsToHistoricalData(startDate,
                                                         endDate,
                                                         archive_folder,
                                                         extension, [])
        if len(historical_files) == 0:
            if extension == '':
                extension = 'Any'
            
            if startDate == '' or startDate is None:
                startDate = 'Any'
                
            if endDate == '' or endDate is None:
                endDate = 'Any'
                
            sys.stderr.write("There are no routing files in the archive that meet the criteria (extension ({}) and period of time ({}-{})).".format(extension, startDate, endDate))
        else:
            self.storeHistoricalData(historical_files)
            sys.stdout.write("Historical data inserted into visibility database successfully!\n")

    
    # This function returns a list of paths to the routing files from the files
    # in the archive corresponding to the provided date
    def getSpecificFilesFromArchive(self, routing_date,
                                    archive_folder='/data/wattle/bgplog',
                                    extension='', routing_files=[]):
        
        # We have to add 1 to the provided date as the files in the
        # archive contain routing data corresponding to the day before to the
        # date specified in the file name.
       
        # TODO Check if this is always the case for bgprib files

#        routing_date = routing_date + timedelta(1)

        if extension == '':
            routing_files = self.getSpecificFilesFromArchive(self, routing_date,
                                                             archive_folder,
                                                             'bgprib.mrt',
                                                             routing_files)
            routing_files = self.getSpecificFilesFromArchive(self, routing_date,
                                                             archive_folder,
                                                             'bgpupd.mrt',
                                                             routing_files)
            return routing_files
        
        else:
            month = str(routing_date.month)
            if len(month) == 1:
                month = '0{}'.format(month)
            
            day = str(routing_date.day)
            if len(day) == 1:
                day = '0{}'.format(day)
            
            routing_folder = '{}/{}/{}/{}'.format(archive_folder, routing_date.year,
                                                    month, day)
                                                    
            try:
                for item in os.listdir(routing_folder):
                    if item.endswith(extension):
                        routing_files.append(os.path.join(routing_folder, item))
                if len(routing_files) == 0 and extension == 'bgprib.mrt':
                    return self.getSpecificFilesFromArchive(routing_date,
                                                            archive_folder,
                                                            'dmp.gz',
                                                            routing_files)
            except OSError:
                # If the archive folder does not have the structure
                # <archive_folder>/<year>/<month>/<day>, we walk the folder
                for root, subdirs, files in os.walk(archive_folder):
                    for filename in files:
                        if filename.endswith(extension) and\
                            self.getDateFromFileName(filename) == routing_date:
                            routing_files.append(os.path.join(root, filename))
            
            return routing_files
        
        
    # This function returns a path to the routing file from the provided list 
    # of historical files corresponding to the provided date
    def getSpecificFilesFromHistoricalList(self, historical_list, routing_date):
        
        routing_files = []
        for hist_file in historical_list:
            file_date = self.getDateFromFileName(hist_file)
            
            # We have to add 1 daye to the provided date as the files in the
            # archive contain routing data corresponding to the day before to the
            # date specified in the file name.
#            if file_date == routing_date + timedelta(1):
#               routing_files.append(hist_file)
               
            # TODO Check if this is always the case for bgprib/bgpupd files
            if file_date == routing_date:
                routing_files.append(hist_file)
    
        return routing_files
        
    # This function returns a path to the most recent file in the provided list 
    # of historical files
    def getMostRecentFromHistoricalList(self, historical_files):
        mostRecentDate = datetime.strptime('1970', '%Y').date()
        mostRecentFiles = []
        
        for hist_file in historical_files:
            file_date = self.getDateFromFileName(hist_file)
            
            if file_date > mostRecentDate:
                # We add 1 to the endDate because the files in the archive
                # have routing data for the day before of the date in the
                # name of the file
                mostRecentDate = file_date
                mostRecentFiles = [hist_file]
            elif file_date == mostRecentDate:
                mostRecentFiles.append(hist_file)
    
        return mostRecentFiles
        
    @staticmethod
    def getDateFromFileName(filename):
        
        dates = re.findall('(?P<year>[1-2][9,0][0,1,8,9][0-9])[-_]*(?P<month>[0-1][0-9])[-_]*(?P<day>[0-3][0-9])',\
                    filename)
                    
        if len(dates) > 0:
            file_date = '{}{}{}'.format(dates[0][0], dates[0][1], dates[0][2])
            return datetime.strptime(file_date, '%Y%m%d').date()
        else:
            return None
    
    # This function stores the routing data from the files listed in the
    # historical_files list skipping the mostRecent routing file provided,
    # as the data contained in this file has already been stored.
    def storeHistoricalData(self, historical_files):
        
        i = 0
        for hist_file in historical_files:
             # If we work with several routing files
            sys.stdout.write("Starting to work with %s\n" % hist_file)

            self.storeHistoricalDataFromFile(hist_file)
                        
            i += 1
            if self.DEBUG and i > 1:
                break

    # This function checks for existing prefixes, origin ASes or middle ASes
    # in the corresponding table for the provided date. If the number of
    # existing rows is not zero and is different from the number of rows
    # to insert, all the rows for that date are removed from the table.
    # The rows will be inserted using the COPY method from Postgres,
    # which will stop if it finds any duplicated row. That's why if the
    # number of rows in the DB is different from the number of rows we have
    # to insert, we remove the existing rows and insert the full set of rows.
    # The number of existing rows is returned.
    @staticmethod
    def prepareTableForInsert(table, routing_date, numOfRows, visibilityDB,
                              DEBUG, output_file):
        start = time()
        if table == 'prefixes':
            existingRows = visibilityDB.getPrefixCountForDate(routing_date)
        
            if existingRows != 0 and existingRows != numOfRows:
                visibilityDB.dropPrefixesForDate(routing_date)
                existingRows = 0
        
        elif table == 'originASes':
            existingRows = visibilityDB.getOriginASCountForDate(routing_date)
        
            if existingRows != 0 and existingRows != numOfRows:
                visibilityDB.dropOriginASesForDate(routing_date)
                existingRows = 0
        
        elif table == 'middleASes':
            existingRows = visibilityDB.getMiddleASCountForDate(routing_date)
        
            if existingRows != 0 and existingRows != numOfRows:
                visibilityDB.dropMiddleASesForDate(routing_date)
                existingRows = 0

        end = time()
        
        if DEBUG:
            with open(output_file, 'a') as output:
                output.write('Check for existing {} for this date and delete records if necessary|{}|seconds.\n'.format(table, end-start))
        
        return existingRows
    


    # This function stores the routing data from the routing_file provided
    # into the visibility database
    def storeHistoricalDataFromFile(self, routing_file):
        prefixes, originASes, middleASes, routing_date =\
                        self.getPrefixesASesAndDate(routing_file)
        
        if self.DEBUG:
            with open(self.output_file, 'a') as output:
                output.write('Prefixes to be inserted|{}\n'.format(len(prefixes)))
                output.write('Origin ASes to be inserted|{}\n'.format(len(originASes)))
                output.write('Middle ASes to be inserted|{}\n'.format(len(middleASes)))

        visibilityDB = VisibilityDBHandler(routing_date)

        existingRows = self.prepareTableForInsert('prefixes',
                                                  routing_date,
                                                  len(prefixes),
                                                  visibilityDB,
                                                  self.DEBUG,
                                                  self.output_file)
        if self.DEBUG:
            self.printUsage(self.output_file)

        start = time()
        if existingRows == 0:
            visibilityDB.storeListOfPrefixesSeen(prefixes, routing_date)
        end = time()
        
        if self.DEBUG:
            with open(self.output_file, 'a') as output:
                output.write('Insert the list of prefixes for this date into the DB|{}|seconds\n'.format(end-start))

            self.printUsage(self.output_file)

        existingRows = self.prepareTableForInsert('originASes',
                                                  routing_date,
                                                  len(originASes),
                                                  visibilityDB,
                                                  self.DEBUG,
                                                  self.output_file)

        if self.DEBUG:        
            self.printUsage(self.output_file)
        
        start = time()
        if existingRows == 0:
            visibilityDB.storeListOfASesSeen(originASes, True, routing_date)
        end = time()
        
        if self.DEBUG:
            with open(self.output_file, 'a') as output:
                output.write('Insert the list of origin ASes for this date into the DB|{}|seconds\n'.format(end-start))
        
            self.printUsage(self.output_file)
            
        existingRows = self.prepareTableForInsert('middleASes',
                                                  routing_date,
                                                  len(middleASes),
                                                  visibilityDB,
                                                  self.DEBUG,
                                                  self.output_file)

        if self.DEBUG:        
            self.printUsage(self.output_file)
            
        start = time()
        if existingRows == 0:
            visibilityDB.storeListOfASesSeen(middleASes, False, routing_date)
        end = time()

        if self.DEBUG:
            with open(self.output_file, 'a') as output:
                output.write('Insert the list of middle ASes for this date into the DB|{}|seconds\n'.format(end-start))

            self.printUsage(self.output_file)
            
        visibilityDB.close()
        
    def cleanListOfASes(self, ases_list):
        start = time()
        cleanListOfASes_list = []
        for asn in ases_list:
            if asn is None or asn == 'nan':
                continue
            # If an ASN is in asdot format we convert it to asplain format
            elif '.' in asn:
                left, right= asn.split('.')
                cleanListOfASes_list.append(str(int(left) * 65536 + int(right)))
            # If the asn contains a bracket ({) or parenthesis ((),
            # it is an as-set, therefore, we split it (leaving the brackets out)
            # and consider each AS separately.
            elif '{' in str(asn):
                cleanListOfASes_list.extend(self.cleanListOfASes(asn.replace('{', '').\
                                                    replace('}', '').split(',')))
            elif '(' in str(asn):
                cleanListOfASes_list.extend(self.cleanListOfASes(asn.replace('(', '').\
                                                    replace(')', '').split(',')))
            else:
                cleanListOfASes_list.append(asn)
        
        cleanListOfASes_list = list(set(cleanListOfASes_list))
        end = time()
        
        if self.DEBUG:
            with open(self.output_file, 'a') as output:
                output.write('Clean the list of ASes|{}|seconds\n'.format(end-start))

        return cleanListOfASes_list

    
    # This function returns a list of prefixes for which the routing_file has
    # announcements, a list of the origin ASes included in the routing_file,
    # a list of the middle ASes included in the routing file
    # and the date of the routing file.
    # The routing file is assumed to include routing data for a single day,
    # therefore the date is taken from the timestamp of the first row in the
    # bgp_df DataFrame.
    def getPrefixesASesAndDate(self, routing_file):
        sys.stdout.write("Getting lists of prefixes, origin ASes and middle ASes from {}\n".format(routing_file))

        start = time()
        if not routing_file.endswith('readable'):
            readable_file_name = self.getReadableFile(routing_file, False)
        else:
            readable_file_name = routing_file
        end = time()
        
        if self.DEBUG:
            with open(self.output_file, 'a') as output:
                output.write('Get a readable file|{}|seconds\n'.format(end-start))
        
            self.printUsage(self.output_file)
        
        if readable_file_name == '':
            return [], [], ''

        start = time()
        bgp_df = pd.read_table(readable_file_name, header=None, sep='|',\
                                index_col=False, usecols=[1,3,5,6,7],\
                                names=['timestamp',\
                                        'peer',\
                                        'prefix',\
                                        'ASpath',\
                                        'origin'])
        end = time()

        if self.DEBUG:        
            with open(self.output_file, 'a') as output:
                output.write('Load the readable file into a DataFrame|{}|seconds\n'.format(end-start))
        
            self.printUsage(self.output_file)
        
        if self.DEBUG:
            bgp_df = bgp_df[0:10]

        if bgp_df.shape[0] > 0:        
            start = time()
            routing_date = datetime.utcfromtimestamp(max(bgp_df['timestamp'])).date()
            
            # To get the origin ASes and middle ASes we split the ASpath column
            paths_parts = bgp_df.ASpath.str.rsplit(' ', n=1, expand=True)
    
            prefixes = set(bgp_df['prefix'])
            originASes = set(paths_parts[1])
            middleASes = set([item for sublist in paths_parts[0].tolist() for item in\
                            str(sublist).split()])
            end = time()
            
            if self.DEBUG:
                with open(self.output_file, 'a') as output:
                    output.write('Get the lists of prefixes, origin ASes and middle ASes from the DataFrame|{}|seconds\n'.format(end-start))
            
                self.printUsage(self.output_file)
            
            return prefixes,\
                self.cleanListOfASes(originASes), self.cleanListOfASes(middleASes),\
                routing_date
        else:
            return {}, {}, {}

    # Reading the readable routing file line by line and char by char takes
    # much longer than loading the routing file into a DataFrame, that's why
    # we stick to the other implementation of this function.
    # The code for this implementation is left here commented as a reference.
#    def getPrefixesASesAndDate(self, routing_file):
#        start = time()
#        if not routing_file.endswith('readable'):
#            readable_file_name = self.getReadableFile(routing_file, False)
#        else:
#            readable_file_name = routing_file
#        end = time()
#        sys.stdout.write('It took {} seconds to get a readable file.\n'.format(end-start))
#        
#        if readable_file_name == '':
#            return [], [], ''
#
#        file_date = None
#        prefixes = set()
#        originASes = set()
#        middleASes = set()
#        
#        start = time()
#        with open(readable_file_name, 'rb') as readable_f:
#            line = readable_f.readline()
#            isFirstLine = True
#            
#            while line:
#                curr_field = ''
#                curr_pos = 0
#                ASes = []
#                curr_asn = ''
#                for char in line:
#                    if curr_pos in [1, 5, 6]:
#                        if curr_pos == 6:
#                            if char == ' ' or char == '|':
#                                ASes.append(curr_asn)
#                                curr_asn = ''
#                            else:
#                                curr_asn = '{}{}'.format(curr_asn, char)
#                                
#                        if char == '|':
#                            if isFirstLine and curr_pos == 1:
#                                isFirstLine = False
#                                file_date = datetime.utcfromtimestamp(float(curr_field)).strftime('%Y%m%d')
#                            elif curr_pos == 5:
#                                prefixes.add(curr_field)
#                            elif curr_pos == 6:
#                                if len(ASes) > 0 :
#                                    middleASes = middleASes.union(set(ASes[0:-1]))
#                                    originASes.add(ASes[-1])
#                            curr_field = ''                            
#                            
#                        else:
#                            curr_field = '{}{}'.format(curr_field, char)
#
#                    if char == '|':
#                        curr_pos += 1
#                        if curr_pos > 6:
#                            break
#
#                line = readable_f.readline()
#
#        end = time()
#        sys.stdout.write('It took {} seconds to get the lists of prefixes, origin ASes and middle ASes from the DataFrame.\n'.format(end-start))
#        
#        return prefixes, originASes, middleASes, file_date

    # Reading the readable routing file line by line and then using split takes
    # longer than loading the routing file into a DataFrame, that's why
    # we stick to the other implementation of this function.
    # The code for this implementation is left here commented as a reference.
#    def getPrefixesASesAndDate(self, routing_file):
#        start = time()
#        if not routing_file.endswith('readable'):
#            readable_file_name = self.getReadableFile(routing_file, False)
#        else:
#            readable_file_name = routing_file
#        end = time()
#        sys.stdout.write('It took {} seconds to get a readable file.\n'.format(end-start))
#        
#        if readable_file_name == '':
#            return [], [], ''
#
#        file_date = None
#        prefixes = set()
#        originASes = set()
#        middleASes = set()
#        
#        start = time()
#        with open(readable_file_name, 'rb') as readable_f:
#            line = readable_f.readline()
#            isFirstLine = True
#            
#            while line:
#                line_parts = line.split('|')
#
#                if isFirstLine:
#                    file_date = line_parts[1]
#                    isFirstLine = False
#                
#                if line_parts[5] != '':
#                    prefixes.add(line_parts[5])
#
#                ASpath_parts = line_parts[6].split(' ')
#
#                if len(ASpath_parts) > 0:
#                    originASes.add(ASpath_parts[-1])
#                    middleASes = middleASes.union(set(ASpath_parts[0:-1]))
#
#                line = readable_f.readline()
#
#        end = time()
#        sys.stdout.write('It took {} seconds to get the lists of prefixes, origin ASes and middle ASes from the DataFrame.\n'.format(end-start))
#        
#        return prefixes, originASes, middleASes, file_date
        
    # This function downloads and processes all the files in the provided list.
    # The boolean variable containsURLs must be True if the files_list is a list
    # of URLs or False if it is a list of paths
    def processMultipleFiles(self, files_list, isList, containsURLs):
        if not isList:
            files_list = open(files_list, 'r')
   
        bgp_df = pd.DataFrame() 
        updates_df = pd.DataFrame()
        ipv4Prefixes_radix = radix.Radix()
        ipv6Prefixes_radix = radix.Radix()
        ipv4_longest_pref = -1
        ipv6_longest_pref = -1
        routingDate = datetime.strptime('1970', '%Y').date()
        updatesDate = datetime.strptime('1970', '%Y').date()
        
        i = 0
        for line in files_list:
            line = line.strip()
            if not line.startswith('#') and line != '':
                # If we work with several routing files
                sys.stdout.write("Starting to work with %s\n" % line)

                # We obtain partial data structures
                if containsURLs:
                    if not line.endswith('readable'):
                        readable_file_name =  self.getReadableFile(line, True)          
                    
                        if readable_file_name == '':
                            continue
                    else:
                        readable_file_name = line.strip()
                    
                else:
                    if not line.endswith('readable'):
                        readable_file_name =  self.getReadableFile(line, False)
                    
                        if readable_file_name == '':
                            continue
                    else:
                        readable_file_name = line.strip()
                
                with open(readable_file_name, 'rb') as readable:
                    first_line = readable.readline()
                    
                if 'TABLE_DUMP' in first_line:
                    file_date, bgp_df, ipv4Prefixes_radix, ipv6Prefixes_radix,\
                        ipv4_longest_pref_partial, ipv6_longest_pref_partial =\
                                self.processReadableDF(readable_file_name,
                                                       bgp_df,
                                                       ipv4Prefixes_radix,
                                                       ipv6Prefixes_radix)
                    if file_date > routingDate:
                        routingDate = file_date
                            
                    if ipv4_longest_pref_partial > ipv4_longest_pref:
                        ipv4_longest_pref = ipv4_longest_pref_partial
                        
                    if ipv6_longest_pref_partial > ipv6_longest_pref:
                        ipv6_longest_pref = ipv6_longest_pref_partial
                        
                elif 'BGP4MP' in first_line:
                    updates_file_date, updates_df =\
                                self.processReadableUpdatesDF(readable_file_name)
            
                    if updates_file_date > updatesDate:
                        updatesDate = updates_file_date
        
            i += 1
            if self.DEBUG and i > 1:
                break

        if not isList:        
            files_list.close()
        
        return routingDate, updatesDate, bgp_df, updates_df,\
                ipv4Prefixes_radix, ipv6Prefixes_radix,\
                ipv4_longest_pref, ipv6_longest_pref
    
    
    def completeRadixesFromRoutingFile(self, r_file, ipv4_radix, ipv6_radix):
        if not r_file.endswith('readable'):
            readable_file_name =  self.getReadableFile(r_file, False)
        else:
            readable_file_name = r_file

        bgp_df = pd.read_table(readable_file_name, header=None, sep='|',\
                                index_col=False, usecols=[1,3,5,6,7],\
                                names=['timestamp',\
                                        'peer',\
                                        'prefix',\
                                        'ASpath',\
                                        'origin'])
        
        if bgp_df.shape[0] > 0:
        
            if self.DEBUG:
                bgp_df = bgp_df[0:10]
            
            for prefix, prefix_subset in bgp_df.groupby('prefix'):
                network = IPNetwork(prefix)
                if network.version == 4:
                    prefixes_radix = ipv4_radix
                    
                else:
                    prefixes_radix = ipv6_radix
                    
                node = prefixes_radix.search_exact(prefix)

                if node is None:
                    node = prefixes_radix.add(prefix)

        return ipv4_radix, ipv6_radix
        
    # This function converts a file containing the output of the 'show ip bgp' command
    # to a file in the same format used for BGPDump outputs
    def convertBGPoutput(self, routing_file):
        output_file_name = '%s/%s.readable' % (self.files_path, '.'.join(routing_file.split('/')[-1].split('.')[:-1]))
        output_file = open(output_file_name, 'w')
        
        i = 0
        # load routing table info  (the next loop does it automatically)
        for entry_n, bgp_entry in enumerate(bgp_rib.BGPRIB.parse_cisco_show_ip_bgp_generator(routing_file)):
            routing_date = bgp_entry[8]
#           date_part = str(file_date)[0:8]
#           time_part = str(file_date)[8:12]
            timestamp = str(timegm(datetime.strptime(routing_date, "%Y%m%d%H%M").timetuple()))
            next_hop = bgp_entry[2]
            prefix = bgp_entry[0]
            as_path = bgp_entry[6]
            
            if as_path:
                nextas = as_path[0]
            else:
            	nextas = ''

            if bgp_entry[7] == 'i':
                origin = "IGP"
            elif bgp_entry[7] == 'e':
                origin = "EGP"
            elif bgp_entry[7] == "?":
                origin = "INCOMPLETE"
            else:
                sys.stderr.write("Found invalid prefix at bgp entry %s, with content %s, on file %s\n" %(entry_n, bgp_entry, routing_file))
            	# ignore this line and continue
                continue

            # save information

            #the order for each line is
            #TABLE_DUMP2|date|B|nexthop|NextAS|prefix|AS_PATH|Origin
            output_file.write('TABLE_DUMP|'+timestamp+'|B|'+next_hop+'|'+nextas+'|'+prefix+'|'+" ".join(as_path)+'|'+origin+'\n')
    
            i += 1
            if self.DEBUG and i > 10:
                break
            
        output_file.close()
        
        return output_file_name
    
    # This function processes a readable file corresponding to an updates
    # (bgpupd) file and return a DataFrame with the summarized routing info:
    # Number of announcements and number of withdraws for each prefix
    # on a certain date
    def processReadableUpdatesDF(self, readable_file_name):     
        filtered_readable = '{}.filtered'.format(readable_file_name)
        
        cmd = shlex.split("grep -v STATE {}".format(readable_file_name))
        with open(filtered_readable, 'w') as filtered:
            p = subprocess.Popen(cmd, stdout=filtered)
            p.wait()
            filtered.flush()
        
        # BGP4MP|1493587722|A|202.12.28.1|4777|2a06:21c0::/29|4777 2497 6939 47869 51942|IGP|::ffff:202.12.28.1|0|0||NAG||

        updates_df = pd.read_csv(filtered_readable, header=None, sep='|',\
                                index_col=False, usecols=[1,2,4,5],\
                                names=['timestamp',\
                                        'upd_type',\
                                        'peerAS',
                                        'prefix'])
                                        
        # We assume all the updates included in the updates file correspond
        # to the same date, but in case they don't, we take the most recent date.
        return datetime.utcfromtimestamp(max(updates_df['timestamp'])).date(),\
                updates_df
 
    # This function processes a readable file with routing info
    # putting all the info into a Data Frame  
    def processReadableDF(self, readable_file_name, existing_bgp_df,
                          ipv4Prefixes_radix, ipv6Prefixes_radix):
        
        routing_date = None
        
        ipv4_longest_prefix = -1
        ipv6_longest_prefix = -1
        
        bgp_df = pd.read_table(readable_file_name, header=None, sep='|',\
                                index_col=False, usecols=[1,3,5,6,7],\
                                names=['timestamp',\
                                        'peer',\
                                        'prefix',\
                                        'ASpath',\
                                        'origin'])
        
        if bgp_df.shape[0] > 0:
        
            if self.DEBUG:
                bgp_df = bgp_df[0:10]

            # We take the most recent date of the routing file as the routing date
            # Typically the routing file will containg routing data for a single date
            routing_timestamp = max(bgp_df['timestamp'])
            routing_date = datetime.utcfromtimestamp(routing_timestamp).date()
            
            ASpath_parts = bgp_df.ASpath.str.rsplit(' ', n=1, expand=True)
            bgp_df['originAS'] = ASpath_parts[1]            
            bgp_df['middleASes'] = ASpath_parts[0]
           
            for prefix, prefix_subset in bgp_df.groupby('prefix'):
                network = IPNetwork(prefix)
                if network.version == 4:
                    if network.prefixlen > ipv4_longest_prefix:
                        ipv4_longest_prefix = network.prefixlen
                    prefixes_radix = ipv4Prefixes_radix
                    
                else:
                    if network.prefixlen > ipv6_longest_prefix:
                        ipv6_longest_prefix = network.prefixlen 
                    prefixes_radix = ipv6Prefixes_radix
                

                ASpaths = set(prefix_subset['ASpath'])
                originASes = set(prefix_subset['originAS'])
                
                node = prefixes_radix.search_exact(prefix)
                if node is None:
                    node = prefixes_radix.add(prefix)
                    node.data['ASpaths'] = ASpaths
                    node.data['OriginASes'] = originASes
                else:
                    node.data['ASpaths'] = node.data['ASpaths'].union(ASpaths)
                    node.data['OriginASes'] = node.data['OriginASes'].union(originASes)
                    
            sys.stdout.write('Finished loading Radix trees with prefixes.\n')
            
            bgp_df = pd.concat([existing_bgp_df, bgp_df])
            
        return routing_date, bgp_df, ipv4Prefixes_radix, ipv6Prefixes_radix,\
                ipv4_longest_prefix, ipv6_longest_prefix

    # This function downloads a routing file if the source provided is a URL
    # If the file is compressed, it is unzipped
    # and finally it is processed using BGPdump if the file is a MRTfile
    # or using the functions provided by get_rib.py is the file contains the
    # output of the 'show ip bgp' command
    # The path to the resulting readable file is returned
    def getReadableFile(self, source, isURL):
    
        source_filename = source.split('/')[-1]
        
        # If a routing file is not provided, download it from the provided URL        
        if isURL:
            routing_file = '%s/%s' % (self.files_path, source_filename)
            get_file(source, routing_file)
            source = routing_file
        
        # If the routing file is compressed we unzip it
        if source.endswith('.gz'):
            output_file = '%s/%s' % (self.files_path,\
                                os.path.splitext(source)[0].split('/')[-1])
            
            with gzip.open(source, 'rb') as gzip_file,\
                open(output_file, 'wb') as output:
                try:
                    output.write(gzip_file.read())
                except IOError:
                    return ''
            
            source = output_file
            source_filename = source.split('/')[-1]

        readable_file_name = '%s/%s.readable' % (self.files_path, os.path.splitext(source_filename)[0])
                
        if not os.path.exists(readable_file_name):
            # If the routing file is a MRT file, we process it using BGPdump
            if source.endswith('mrt'):            
                cmd = shlex.split('%s -m -O %s %s' % (bgpdump, readable_file_name, source))
                #        cmd = shlex.split('bgpdump -m -O %s %s' % (readable_file_name, routing_file))   
        
                #  BGPDUMP
                #  -m         one-line per entry with unix timestamps
                #  -O <file>  output to <file> instead of STDOUT
        
                subprocess.call(cmd)
            
            # If the file contains the output of the 'show ip bgp' command,
            # we convert it to the same format used by BGPdump for its outputs
            else:
                readable_file_name = self.convertBGPoutput(source)

        return readable_file_name
           
    # This function walks a folder with historical routing info and creates a
    # file with a list of paths to the files with the provided extension
    # in the archive folder. If an extension is not provided, 'bgprib.mrt' is used.
    # It returns the path to the created file
    def getPathsToHistoricalData(self, startDate, endDate,
                                 archive_folder='/data/wattle/bgplog',
                                 extension='', files_list=[]):

        if extension == '':
            files_list = self.getPathsToHistoricalData(startDate, endDate,
                                                       archive_folder,
                                                       'bgprib.mrt',
                                                       files_list)
            files_list = self.getPathsToHistoricalData(startDate, endDate,
                                                       archive_folder,
                                                       'bgpupd.mrt',
                                                       files_list)
            return files_list

        else:        
            for root, subdirs, files in os.walk(archive_folder):
                for filename in files:
                    if filename.endswith(extension):
                        file_date = self.getDateFromFileName(filename)
                        if (endDate != '' and file_date <= endDate + timedelta(1) or endDate == '') and\
                            (startDate != '' and file_date > startDate or startDate == ''):
                            files_list.append(os.path.join(root, filename))
            
            if len(files_list) == 0:
                if extension == 'bgprib.mrt':
                    return self.getPathsToHistoricalData(startDate, endDate,
                                                         archive_folder,
                                                         'dmp.gz', files_list)
                else:
                    return []
            else:
                return sorted(files_list)
    
    def getMostRecentsFromArchive(self, archive_folder, extension, files_list):
        if extension == '':
            files_list = self.getMostRecentsFromArchive(archive_folder,
                                                        'bgprib.mrt',
                                                        files_list)
            files_list = self.getMostRecentsFromArchive(archive_folder,
                                                        'bgpupd.mrt',
                                                        files_list)
            return files_list
            
        else:
            mostRecentDate = datetime.strptime('1970', '%Y').date()
            
            for root, subdirs, files in os.walk(archive_folder):
                for filename in files:
                    if filename.endswith(extension):
                        file_date = self.getDateFromFileName(filename)
                        if file_date > mostRecentDate:
                            files_list = [os.path.join(root, filename)]
                            mostRecentDate = file_date
                        elif file_date == mostRecentDate:
                            files_list.append(os.path.join(root, filename))
            
            if len(files_list) == 0:
                if extension == 'bgprib.mrt':
                    return self.getMostRecentsFromArchive(archive_folder,
                                                          'dmp.gz', files_list)
                else:
                    return []
            else:
                return files_list


    # This function walks a folder with historical routing info and returns
    # a dictionary indexed by date containing the paths to the files
    # in the archive folder for the corresponding date that have the provided
    # extension. If no extension is provided, ALL the files in the archive
    # for each date are included in the dict
    # (bgprib.mrt, dmp.gz and bgpupd.mrt files)
    def getPathsToHistoricalData_dict(self, startDate, endDate,
                                         archive_folder, extension='',
                                         files_dict=[]):
                                             
        if extension == '':
            files_dict = self.getPathsToHistoricalData_dict(startDate, endDate,
                                                            archive_folder,
                                                            'bgprib.mrt',
                                                            files_dict)
            
            files_dict = self.getPathsToHistoricalData_dict(startDate, endDate,
                                                            archive_folder,
                                                            'dmp.gz',
                                                            files_dict)
                                                            
            files_dict = self.getPathsToHistoricalData_dict(startDate, endDate,
                                                            archive_folder,
                                                            'bgpupd.mrt',
                                                            files_dict)
            return files_dict
        else:
            for root, subdirs, files in os.walk(archive_folder):
                for filename in files:
                    if filename.endswith(extension):
                        file_date = self.getDateFromFileName(filename)
                        if (endDate != '' and file_date <= endDate + timedelta(1) or\
                            endDate == '') and (startDate != '' and\
                            file_date > startDate or startDate == ''):
                                file_path = os.path.join(root, filename)

                                if file_date not in files_dict:
                                    files_dict[file_date] = [file_path]
                                else:
                                    files_dict[file_date].append(file_path)
    
            return files_dict

    # This function saves the data structures of the class to pickle files
    def saveDataToFiles(self):
        today = date.today().strftime('%Y%m%d')
        
        ipv4_radix_file_name = '%s/ipv4Prefixes_%s.pkl' % (self.files_path, today)
        with open(ipv4_radix_file_name, 'wb') as f:
            pickle.dump(self.ipv4Prefixes_radix, f, pickle.HIGHEST_PROTOCOL)
            sys.stdout.write("Saved to disk %s pickle file containing Radix with routing data for each IPv4 prefix.\n" % ipv4_radix_file_name)

        ipv6_radix_file_name = '%s/ipv6Prefixes_%s.pkl' % (self.files_path, today)
        with open(ipv6_radix_file_name, 'wb') as f:
            pickle.dump(self.ipv6Prefixes_radix, f, pickle.HIGHEST_PROTOCOL)
            sys.stdout.write("Saved to disk %s pickle file containing Radix with routing data for each IPv6 prefix.\n" % ipv6_radix_file_name)

        return ipv4_radix_file_name, ipv6_radix_file_name

    # This function sets the ipv4_longest_pref and ipv6_longest_pref class variables
    # with the corresponding maximum prefix lengths in the ipv4_prefixes_indexes
    # and ipv6_prefixes_indexes Radixes
    def setLongestPrefixLengths(self):
        for prefix in self.ipv4_prefixes_indexes_radix.prefixes():
            network = IPNetwork(prefix)
            
            if network.prefixlen > self.ipv4_longest_pref:
                self.ipv4_longest_pref = network.prefixlen
                
        for prefix in self.ipv6_prefixes_indexes_radix.prefixes():
            network = IPNetwork(prefix)

            if network.prefixlen > self.ipv6_longest_pref:
                self.ipv6_longest_pref = network.prefixlen
                
    # This function returns a list of prefixes less specific than the one provided
    # that are included in the keys of the corresponding Radix
    def getRoutedParentAndGrandparents(self, network):        
        if network.version == 4:
            prefixes_radix = self.ipv4Prefixes_radix
        else:
            prefixes_radix = self.ipv6Prefixes_radix
            
        less_specifics = []
       
        for less_spec_node in prefixes_radix.search_covering(str(network)):
            less_spec_pref = less_spec_node.prefix
        
            if less_spec_pref != str(network):
                less_specifics.append(less_spec_pref)
            
        return less_specifics
    
    # This function returns a list of prefixes more specific than the one provided
    # that are included in the keys of the corresponding Radix
    def getRoutedChildren(self, network):
        if network.version == 4:
            prefixes_radix = self.ipv4Prefixes_radix
        else:
            prefixes_radix = self.ipv6Prefixes_radix
            
        more_specifics = []
       
        for more_spec_node in prefixes_radix.search_covered(str(network)):
            more_specifics.append(more_spec_node.prefix)
                        
        return more_specifics
        
    # This function returns the origin AS for a specific prefix
    # according to the routing data included in the BGP_data class variable
    def getOriginASesForBlock(self, network):        
        if network.version == 4:
            prefixes_radix = self.ipv4Prefixes_radix
        else:
            prefixes_radix = self.ipv6Prefixes_radix

        pref_node = prefixes_radix.search_exact(str(network))
        if pref_node is not None:
            return set(pref_node.data['OriginASes'])
        else:
            return set()
    
    # This function returns a set with all the AS paths for a specific prefix
    # according to the routing data included in the BGP_data class variable
    def getASpathsForBlock(self, network):
        if network.version == 4:
            prefixes_radix = self.ipv4Prefixes_radix
        else:
            prefixes_radix = self.ipv6Prefixes_radix
            
        pref_node = prefixes_radix.search_exact(str(network))
        if pref_node is not None:
            return pref_node.data['ASpaths']
        else:
            return set()
    
    
    @staticmethod
    def printUsage(output_file):
        usage = resource.getrusage(resource.RUSAGE_SELF)
    
        '''
        Index	Field	Resource
        0	ru_utime	time in user mode (float)
        1	ru_stime	time in system mode (float)
        2	ru_maxrss	maximum resident set size
        3	ru_ixrss	shared memory size
        4	ru_idrss	unshared memory size
        5	ru_isrss	unshared stack size
        6	ru_minflt	page faults not requiring I/O
        7	ru_majflt	page faults requiring I/O
        8	ru_nswap	number of swap outs
        9	ru_inblock	block input operations
        10	ru_oublock	block output operations
        11	ru_msgsnd	messages sent
        12	ru_msgrcv	messages received
        13	ru_nsignals	signals received
        14	ru_nvcsw	voluntary context switches
        15	ru_nivcsw	involuntary context switches
        '''
    
        with open(output_file, 'a') as output:
            for name, desc in [
                ('ru_utime', 'User time'),
                ('ru_stime', 'System time'),
                ('ru_maxrss', 'Max. Resident Set Size'),
                ('ru_ixrss', 'Shared Memory Size'),
                ('ru_idrss', 'Unshared Memory Size'),
                ('ru_isrss', 'Stack Size'),
                ('ru_nswap', 'Num. of Swap outs'),
                ('ru_inblock', 'Block inputs'),
                ('ru_oublock', 'Block outputs'),
                ]:
                output.write('{} ({})|{}\n'.format(desc, name, getattr(usage, name)))
